{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8318937,
          "sourceType": "datasetVersion",
          "datasetId": 4941176
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook3489556efe",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'medical-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4941176%2F8318937%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240506%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240506T153315Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D76781938fe31eed80ea65b95c4b0bc5c050d25771eddcb9a826a33da86984878f4db594f760515932e6291f4f781e9e788f124f9c973935c6ee23f793482d0b38fc4bf123ee2f03f5dc0427f572dff7f7c84c0ced6f72bf092e2f51dee08fca9a8eeede001596bb5fab98ac7de5f5bced29ce1a78beaf07e5746b6d7db4581d284e625045a4010c83cc4cd077debaaedcfa805164a3fffa5fc848173523c62174407aa952590cf59b1658de3ef6fca783ddde9e1294add8b03bcf295493aea053d403553d1ed7380ccfb75904ec29c8e47fa9fbb6e7ac9a355732a2a952479a13c249f105da5458df37094ae193c60fc34e3ffb3f2c0e81402c35bb00aa9528b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "aaDDucRwib9u"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ihb9XfQoib9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Specify the path to your dataset\n",
        "dataset_path = '/kaggle/input/medical-text/data.csv'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"Dataset found.\")\n",
        "else:\n",
        "    print(\"Dataset not found. Check the file path.\")\n",
        "\n",
        "    from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('csv', data_files=dataset_path)['train']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-06T14:49:52.059329Z",
          "iopub.execute_input": "2024-05-06T14:49:52.059706Z",
          "iopub.status.idle": "2024-05-06T14:49:54.266025Z",
          "shell.execute_reply.started": "2024-05-06T14:49:52.059675Z",
          "shell.execute_reply": "2024-05-06T14:49:54.265122Z"
        },
        "trusted": true,
        "id": "FkGZn36Eib9y",
        "outputId": "195ccffd-eb86-4bc7-ac22-ee678d6e30f4",
        "colab": {
          "referenced_widgets": [
            "d4374c427656432f80c5f3c593a91a78"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset found.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4374c427656432f80c5f3c593a91a78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the tokenizer and model for BERT (you can switch this to DistilBERT or any other model)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFAutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "test_val_split = train_test_split['test'].train_test_split(test_size=0.5)\n",
        "split_dataset = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'validation': test_val_split['test'],\n",
        "    'test': test_val_split['train']\n",
        "})\n",
        "\n",
        "# Tokenization and converting to TensorFlow format\n",
        "def tokenize_and_format(batch):\n",
        "    tokenized_batch = tokenizer(batch['medical_abstract'], padding='max_length', truncation=True, max_length=512)\n",
        "    tokenized_batch = {k: tf.convert_to_tensor(v) for k, v in tokenized_batch.items()}\n",
        "    tokenized_batch['labels'] = tf.convert_to_tensor(batch['condition_label'])\n",
        "    return tokenized_batch\n",
        "\n",
        "split_dataset = split_dataset.map(tokenize_and_format, batched=True)\n",
        "split_dataset.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Function to prepare inputs for BERT\n",
        "def map_example_to_dict(input_ids, attention_mask, labels):\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "    }, labels\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "batch_size = 16\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((split_dataset['train']['input_ids'],\n",
        "                                                    split_dataset['train']['attention_mask'],\n",
        "                                                    split_dataset['train']['labels']))\n",
        "train_dataset = train_dataset.map(map_example_to_dict).shuffle(1000).batch(batch_size)\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((split_dataset['validation']['input_ids'],\n",
        "                                                         split_dataset['validation']['attention_mask'],\n",
        "                                                         split_dataset['validation']['labels']))\n",
        "validation_dataset = validation_dataset.map(map_example_to_dict).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((split_dataset['test']['input_ids'],\n",
        "                                                   split_dataset['test']['attention_mask'],\n",
        "                                                   split_dataset['test']['labels']))\n",
        "test_dataset = test_dataset.map(map_example_to_dict).batch(batch_size)\n",
        "\n",
        "# Define the TensorFlow model\n",
        "class BERTForClassification(tf.keras.Model):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        # In case of using BERT, `bert_model.pooler_output` might be used or use `bert_model.last_hidden_state`\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.bert(inputs)\n",
        "        # Use pooled output for classification tasks\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# Initialize the model\n",
        "num_classes = 6  # Adjust based on the number of condition labels\n",
        "classifier = BERTForClassification(model, num_classes=num_classes)\n",
        "\n",
        "# Compile the model\n",
        "classifier.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "history = classifier.fit(train_dataset, epochs=3, validation_data=validation_dataset)\n",
        "\n",
        "# Evaluate the model\n",
        "test_results = classifier.evaluate(test_dataset)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-06T14:49:54.267565Z",
          "iopub.execute_input": "2024-05-06T14:49:54.268055Z",
          "iopub.status.idle": "2024-05-06T15:28:01.311348Z",
          "shell.execute_reply.started": "2024-05-06T14:49:54.268026Z",
          "shell.execute_reply": "2024-05-06T15:28:01.31047Z"
        },
        "trusted": true,
        "id": "EDCtnqUlib9z",
        "outputId": "6d60c9bb-d67c-4e18-c8b8-8cf622fd16b6",
        "colab": {
          "referenced_widgets": [
            "8d562b7b60284d7498b8370dfe7889e3",
            "d5557566b98143e28e82584ea46db97c",
            "a9ca1e162c5c452d8e9982ad0491f464",
            "cbd2989cbbc444aa9bb9b08490bca413",
            "68e32aebf11345df9bdb4a916f451c4b",
            "5314671f45064997be160c356674d223",
            "2a0e6daa339f4ee8ab41d35e69bed663",
            "7cb60dcce7b84c53829190b937308bcc"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-05-06 14:49:55.921088: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 14:49:55.921207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 14:49:56.051861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d562b7b60284d7498b8370dfe7889e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5557566b98143e28e82584ea46db97c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9ca1e162c5c452d8e9982ad0491f464"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbd2989cbbc444aa9bb9b08490bca413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68e32aebf11345df9bdb4a916f451c4b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10395 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5314671f45064997be160c356674d223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/578 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a0e6daa339f4ee8ab41d35e69bed663"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/577 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cb60dcce7b84c53829190b937308bcc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 1/3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715007101.081807     116 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "650/650 [==============================] - 812s 1s/step - loss: 1.0093 - accuracy: 0.5828 - val_loss: 0.8858 - val_accuracy: 0.6298\nEpoch 2/3\n650/650 [==============================] - 715s 1s/step - loss: 0.7981 - accuracy: 0.6586 - val_loss: 0.8827 - val_accuracy: 0.6211\nEpoch 3/3\n650/650 [==============================] - 714s 1s/step - loss: 0.6990 - accuracy: 0.6925 - val_loss: 0.8997 - val_accuracy: 0.6107\n37/37 [==============================] - 13s 356ms/step - loss: 0.8627 - accuracy: 0.5997\nTest results - Loss: 0.8626721501350403 - Accuracy: 59.965336322784424%\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}